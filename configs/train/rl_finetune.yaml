PPO:
  model_path: ../../DeepSeek-R1-Distill-Qwen-1.5B
  tokenizer_path: ../../DeepSeek-R1-Distill-Qwen-1.5B
  sft_model_path: ../../DeepSeek-R1-Distill-Qwen-1.5B
  rm_model_path: "path/to/your/reward_model"
  output_dir: "outputs/ppo_finetune"

  train_file_path: "path/to/your/train_data.json"

  learning_rate: 1.0e-5
  num_epochs: 3
  batch_size: 4 # useless

  response_length: 128
  kl_coef: 0.1
  ppo_epochs: 4
  clip_grad_norm: 1.0
  log_dir: ../logs
  num_workers: 8
  max_input_length: 1024
  gradient_accumulation_steps: 2
  pipeline:
    train:
      seed: 42
      logging_epoch_freq: 1
      saving_epoch_freq: 1
      train_file: ../../data/LeetCodeDataset_train_cleaned.jsonl
      batch_size: 3
      n_epochs: 10
      clip_grad_norm: 1.0
      optimizer:
        learning_rate: 1.0e-5
        weight_decay: 0
    test:
      evaluating_epoch_freq: 1
      test_file: ../../data/LeetCodeDataset_test_cleaned.jsonl
      batch_size: 3
  lora:
    r: 4
    lora_alpha: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      # - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM