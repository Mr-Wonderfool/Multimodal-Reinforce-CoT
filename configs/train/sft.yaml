SFT:
  model_path: ../../../qwen/Qwen2.5-VL-3B-Instruct
  tokenizer_path: ../../../qwen/Qwen2.5-VL-3B-Instruct
  log_dir: ../../logs
  image_dir: ../../../data/multimodal_cot/images
  num_workers: 4
  max_input_length: 2048
  max_response_length: 1024
  gradient_accumulation_steps: 2
  # whether to apply quantization
  quantization: true

  pipeline:
    train:
      seed: 42
      logging_epoch_freq: 1
      saving_epoch_freq: 6
      train_file: ../../../data/multimodal_cot/train_data.jsonl
      batch_size: 1
      n_epochs: 12
      clip_grad_norm: 1.0
      optimizer:
        learning_rate: 1.0e-5
        weight_decay: 0
    val:
      val_file: ../../../data/multimodal_cot/dev_data.jsonl
      batch_size: 1
    test:
      test_file: ../../../data/multimodal_cot/test.jsonl
      batch_size: 1
      evaluating_epoch_freq: 1

  lora:
    r: 4
    lora_alpha: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      # - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LMs