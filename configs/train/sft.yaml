SFT:
  model_path: ../../DeepSeek-R1-Distill-Qwen-1.5B
  tokenizer_path: ../../DeepSeek-R1-Distill-Qwen-1.5B
  log_dir: ../logs
  num_workers: 4
  max_input_length: 2048
  max_response_length: 1024
  gradient_accumulation_steps: 2
  # whether to apply quantization
  quantization: true

  pipeline:
    train:
      seed: 42
      logging_epoch_freq: 1
      saving_epoch_freq: 6
      train_file: ../../data/LeetCodeDataset_train_cleaned.jsonl
      batch_size: 1
      n_epochs: 12
      clip_grad_norm: 1.0
      optimizer:
        learning_rate: 1.0e-5
        weight_decay: 0
    test:
      evaluating_epoch_freq: 1
      test_file: ../../data/LeetCodeDataset_test_cleaned.jsonl
      batch_size: 1

  lora:
    r: 4
    lora_alpha: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      # - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM