SFT:
  model_path: ../../Qwen2.5-VL-3B-Instruct
  tokenizer_path: ../../Qwen2.5-VL-3B-Instruct
  log_dir: ../logs
  num_workers: 4
  max_input_length: 2048
  # max_response_length: 1024
  gradient_accumulation_steps: 2
  # whether to apply quantization
  quantization: true

  pipeline:
    train:
      seed: 42
      logging_epoch_freq: 1
      saving_epoch_freq: 6
      train_file: train_dataset/train_data.jsonl
      image_dir: train_dataset/images 
      batch_size: 2
      n_epochs: 12
      clip_grad_norm: 1.0
      optimizer:
        learning_rate: 1.0e-5
        weight_decay: 0
    val:                                             
      val_file: train_dataset/dev_data.jsonl
      image_dir: train_dataset/images
      batch_size: 2
    test:
      test_file: train_dataset/test_data.jsonl
      image_dir: train_dataset/images
      batch_size: 2

  lora:
    r: 4
    lora_alpha: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      # - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM