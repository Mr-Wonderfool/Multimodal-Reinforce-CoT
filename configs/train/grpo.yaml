GRPO:
  model_path: ../../../qwen/qwen-merged-sft
  tokenizer_path: ../../../qwen/qwen-merged-sft
  # for evaluation only
  lora_adapter_path: ../../../qwen/final_checkpoint
  image_dir: ../../../data/multimodal_cot/images
  test_image_dir: ../../../data/multimodal_cot/test_images
  # directory for custom logging
  log_dir: ../../logs

  # it seems quantization is not compatible with vllm
  quantization: false

  # ------ parameter adopted from trl.GRPOConfig ------
  dataloader_num_workers: 4
  report_to: tensorboard
  output_dir: ../../../qwen/
  logging_dir: ../../logs

  max_prompt_length: 2048
  max_completion_length: 768
  # ! sync this with deepspeed configuration
  gradient_accumulation_steps: 2
  # whether use page attention to speed up inference
  use_vllm: true
  vllm_mode: colocate

  # avoid messing with custom dataset
  remove_unused_columns: false

  vllm_gpu_memory_utilization: 0.6

  pipeline:
    train:
      train_file: ../../../data/multimodal_cot/train_data.jsonl
      do_train: true
      per_device_train_batch_size: 2
      seed: 42
      logging_steps: 10
      log_level: info
      # total number of training steps
      num_train_epochs: 5
      # number of iterations for minimizing the objective
      num_iterations: 8
      # number of groups, should be factor of effective batch size
      num_generations: 6
      # number of batch data to sample per training iteration
      steps_per_generation: 6
      # KL weight, if set to 0.0 the reference model will not be loaded
      beta: 0.05
      # importance sampling level, can be 'token' or 'sequence'
      importance_sampling_level: sequence
      # whether divide reward by std
      scale_rewards: true
      # loss type, 'bnpo' or 'dr_grpo'
      loss_type: bnpo
      # reference model configuration
      sync_ref_model: false
      # ratio of training before saving the model
      save_steps: 0.4
      optimizer:
        learning_rate: 1.0e-5
        weight_decay: 0.01
    val:
      do_eval: false
      val_file: ../../../data/multimodal_cot/dev_data.jsonl
    test:
      do_predict: false
      test_file: ../../../data/multimodal_cot/test.jsonl
      batch_size: 8
      evaluating_epoch_freq: 1

  # ! this section will be discarded if training continues on SFT weights
  lora:
    r: 64
    lora_alpha: 128
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      # - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM